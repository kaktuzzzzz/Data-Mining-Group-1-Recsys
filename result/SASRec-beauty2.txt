(bert4rec2) tuhpcc@pc-18:~/tu2/bert4rec2/sasrec-bert4rec-recsys23/src$ python run.py --config-name=SASRec data_path=../data/beauty2.txt +seqrec_module.loss=bce +dataset.num_negatives=1 dataset.full_negative_sampling=True
cuda_visible_devices: 0
data_path: ../data/beauty2.txt
dataset:
  max_length: 50
  full_negative_sampling: true
  num_negatives: 1
dataloader:
  batch_size: 128
  test_batch_size: 256
  num_workers: 8
  validation_size: 10000
model: SASRec
model_params:
  maxlen: 200
  hidden_units: 64
  num_blocks: 2
  num_heads: 1
  dropout_rate: 0.2
seqrec_module:
  lr: 0.001
  predict_top_k: 10
  filter_seen: true
  loss: bce
trainer_params:
  max_epochs: 100
patience: 10
sampled_metrics: false
top_k_metrics:
- 1
- 10
- 100

[2025-04-17 03:40:11,182][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp5v1k4gqb
[2025-04-17 03:40:11,183][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp5v1k4gqb/_remote_module_non_scriptable.py
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                              | Type                            | Params
---------------------------------------------------------------------------------------
0  | model                             | SASRec                          | 1.5 M 
1  | model.item_emb                    | Embedding                       | 1.4 M 
2  | model.pos_emb                     | Embedding                       | 12.8 K
3  | model.emb_dropout                 | Dropout                         | 0     
4  | model.attention_layernorms        | ModuleList                      | 256   
5  | model.attention_layernorms.0      | LayerNorm                       | 128   
6  | model.attention_layernorms.1      | LayerNorm                       | 128   
7  | model.attention_layers            | ModuleList                      | 33.3 K
8  | model.attention_layers.0          | MultiheadAttention              | 16.6 K
9  | model.attention_layers.0.out_proj | NonDynamicallyQuantizableLinear | 4.2 K 
10 | model.attention_layers.1          | MultiheadAttention              | 16.6 K
11 | model.attention_layers.1.out_proj | NonDynamicallyQuantizableLinear | 4.2 K 
12 | model.forward_layernorms          | ModuleList                      | 256   
13 | model.forward_layernorms.0        | LayerNorm                       | 128   
14 | model.forward_layernorms.1        | LayerNorm                       | 128   
15 | model.forward_layers              | ModuleList                      | 16.6 K
16 | model.forward_layers.0            | PointWiseFeedForward            | 8.3 K 
17 | model.forward_layers.0.conv1      | Conv1d                          | 4.2 K 
18 | model.forward_layers.0.dropout1   | Dropout                         | 0     
19 | model.forward_layers.0.relu       | ReLU                            | 0     
20 | model.forward_layers.0.conv2      | Conv1d                          | 4.2 K 
21 | model.forward_layers.0.dropout2   | Dropout                         | 0     
22 | model.forward_layers.1            | PointWiseFeedForward            | 8.3 K 
23 | model.forward_layers.1.conv1      | Conv1d                          | 4.2 K 
24 | model.forward_layers.1.dropout1   | Dropout                         | 0     
25 | model.forward_layers.1.relu       | ReLU                            | 0     
26 | model.forward_layers.1.conv2      | Conv1d                          | 4.2 K 
27 | model.forward_layers.1.dropout2   | Dropout                         | 0     
28 | model.last_layernorm              | LayerNorm                       | 128   
---------------------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.000     Total estimated model params size (MB)
Epoch 27: 100%|█████████████████████████████████████████████████████████████████████| 260/260 [00:08<00:00, 32.39it/s, loss=0.11, v_num=5, val_ndcg=0.0262, val_hit_rate=0.0512, val_mrr=0.0187]
training_time 230.32457447052002                                                                                                                                                                
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Predicting DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [00:09<00:00, -11.70it/s]
recs shape (2804000, 3)
{'val_ndcg@1': 0.010057061340941512, 'val_hit_rate@1': 0.010057061340941512, 'val_mrr@1': 0.010057061340941512}
{'val_ndcg@10': 0.026732915769849656, 'val_hit_rate@10': 0.049572039942938656, 'val_mrr@10': 0.019823084369268395}
{'val_ndcg@100': 0.05066534389915841, 'val_hit_rate@100': 0.1720756062767475, 'val_mrr@100': 0.023821709313725382}
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Predicting DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [00:09<00:00, -11.72it/s]
recs shape (2804000, 3)
{'test_ndcg@1': 0.008059914407988587, 'test_hit_rate@1': 0.008059914407988587, 'test_mrr@1': 0.008059914407988587}
{'test_ndcg@10': 0.0212509574558172, 'test_hit_rate@10': 0.03955064194008559, 'test_mrr@10': 0.015728466136811357}
{'test_ndcg@100': 0.04167556537516751, 'test_hit_rate@100': 0.14465049928673324, 'test_mrr@100': 0.019097052778285953}