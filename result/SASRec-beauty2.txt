(bert4rec2) tuhpcc@pc-18:~/Datamining-CN01-Group1/src$ python run.py --config-name=SASRec data_path=../data/beauty2.txt +seqrec_module.loss=bce +dataset.num_negatives=1 dataset.full_negative_sampling=True
cuda_visible_devices: 0
data_path: ../data/beauty2.txt
dataset:
  max_length: 50
  full_negative_sampling: true
  num_negatives: 1
dataloader:
  batch_size: 128
  test_batch_size: 256
  num_workers: 8
  validation_size: 10000
model: SASRec
model_params:
  maxlen: 200
  hidden_units: 64
  num_blocks: 2
  num_heads: 1
  dropout_rate: 0.5
seqrec_module:
  lr: 0.001
  predict_top_k: 10
  filter_seen: true
  loss: bce
trainer_params:
  max_epochs: 100
patience: 10
sampled_metrics: false
top_k_metrics:
- 1
- 10
- 100

[2025-04-17 10:20:09,321][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp25aq70sy
[2025-04-17 10:20:09,322][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp25aq70sy/_remote_module_non_scriptable.py
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                              | Type                            | Params
---------------------------------------------------------------------------------------
0  | model                             | SASRec                          | 1.5 M 
1  | model.item_emb                    | Embedding                       | 1.4 M 
2  | model.pos_emb                     | Embedding                       | 12.8 K
3  | model.emb_dropout                 | Dropout                         | 0     
4  | model.attention_layernorms        | ModuleList                      | 256   
5  | model.attention_layernorms.0      | LayerNorm                       | 128   
6  | model.attention_layernorms.1      | LayerNorm                       | 128   
7  | model.attention_layers            | ModuleList                      | 33.3 K
8  | model.attention_layers.0          | MultiheadAttention              | 16.6 K
9  | model.attention_layers.0.out_proj | NonDynamicallyQuantizableLinear | 4.2 K 
10 | model.attention_layers.1          | MultiheadAttention              | 16.6 K
11 | model.attention_layers.1.out_proj | NonDynamicallyQuantizableLinear | 4.2 K 
12 | model.forward_layernorms          | ModuleList                      | 256   
13 | model.forward_layernorms.0        | LayerNorm                       | 128   
14 | model.forward_layernorms.1        | LayerNorm                       | 128   
15 | model.forward_layers              | ModuleList                      | 16.6 K
16 | model.forward_layers.0            | PointWiseFeedForward            | 8.3 K 
17 | model.forward_layers.0.conv1      | Conv1d                          | 4.2 K 
18 | model.forward_layers.0.dropout1   | Dropout                         | 0     
19 | model.forward_layers.0.relu       | ReLU                            | 0     
20 | model.forward_layers.0.conv2      | Conv1d                          | 4.2 K 
21 | model.forward_layers.0.dropout2   | Dropout                         | 0     
22 | model.forward_layers.1            | PointWiseFeedForward            | 8.3 K 
23 | model.forward_layers.1.conv1      | Conv1d                          | 4.2 K 
24 | model.forward_layers.1.dropout1   | Dropout                         | 0     
25 | model.forward_layers.1.relu       | ReLU                            | 0     
26 | model.forward_layers.1.conv2      | Conv1d                          | 4.2 K 
27 | model.forward_layers.1.dropout2   | Dropout                         | 0     
28 | model.last_layernorm              | LayerNorm                       | 128   
---------------------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.000     Total estimated model params size (MB)
Epoch 37: 100%|█████████████████████████████████████████████████████████████████████| 260/260 [00:08<00:00, 31.53it/s, loss=0.179, v_num=9, val_ndcg=0.0286, val_hit_rate=0.055, val_mrr=0.0207]
training_time 311.8424973487854                                                                                                                                                                 
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Predicting DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [00:09<00:00, -11.72it/s]
recs shape (2804000, 3)
{'val_ndcg@1': 0.009914407988587733, 'val_hit_rate@1': 0.009914407988587733, 'val_mrr@1': 0.009914407988587733}
{'val_ndcg@10': 0.028708851367963112, 'val_hit_rate@10': 0.05470756062767475, 'val_mrr@10': 0.020850273418925345}
{'val_ndcg@100': 0.05340274402797712, 'val_hit_rate@100': 0.1807061340941512, 'val_mrr@100': 0.025018028607876525}
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Predicting DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [00:09<00:00, -11.76it/s]
recs shape (2804000, 3)
{'test_ndcg@1': 0.008202567760342368, 'test_hit_rate@1': 0.008202567760342368, 'test_mrr@1': 0.008202567760342368}
{'test_ndcg@10': 0.023304948697116103, 'test_hit_rate@10': 0.044365192582025675, 'test_mrr@10': 0.016951860154427914}
{'test_ndcg@100': 0.044896726663276446, 'test_hit_rate@100': 0.15524251069900144, 'test_mrr@100': 0.02053045284638784}